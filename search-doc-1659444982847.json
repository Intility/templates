[{"title":"React","type":0,"sectionRef":"#","url":"cra/","content":"","keywords":""},{"title":"Create React App​","type":1,"pageTitle":"React","url":"cra/#create-react-app","content":"Create React App is the main part of the development environment, these docs only covers the Intility templates for it. This means that everything from the official Create React App docs applies to your project. If you can't find what your looking for here, check the CRA docs. "},{"title":"Recommended reading​","type":1,"pageTitle":"React","url":"cra/#recommended-reading","content":"Environment VariablesAdvanced ConfigurationCode Splitting "},{"title":"Deploy with ArgoCD","type":0,"sectionRef":"#","url":"cra/advanced/deploy-argocd","content":"Deploy with ArgoCD info This page isn't finished yet. If you are comfortable with this subject, feel free to contribute.","keywords":""},{"title":"Adding Environments","type":0,"sectionRef":"#","url":"cra/advanced/adding-environments","content":"","keywords":""},{"title":"Enabling the deploy:prod job​","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#enabling-the-deployprod-job","content":"The provided deploy:prod job is disabled by default. This is to force you to make an active choice on wether you want to use the job or ArgoCD. To enable the job, simply remove the . from the last job. .gitlab-ci.yml - .deploy:prod: + deploy:prod:  "},{"title":"Trigger production jobs​","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#trigger-production-jobs","content":"To trigger the production jobs, simply run npm version [major|minor|patch] git push --tags  This will update the version field in package.json, update the appVersion field in chart/Chart.yaml, and create a git tag with the version. After all the other jobs has completed, we can manually trigger the deploy:prod job. This is manual by design, and shouldn't be set to automatic. "},{"title":"Separate OpenShift project​","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#separate-openshift-project","content":"You can, and should create a separate OpenShift project to host the production environment. To do this, simply follow the same steps as earlier. This time, name it aa-&lt;project-slug&gt;-prod. When adding the OPENSHIFT_TOKEN to CI/CD variables, select prod as the Environment scope. This variable will then be used instead of the default one in the deploy:prod job.  note If you can't see the prod environment when creating a variable, you need to trigger the production jobs first. The build:prod job will then provision the prod environment, and you can insert the OPENSHIFT_TOKEN before triggering the deploy job. "},{"title":"Adding even more Environments​","type":1,"pageTitle":"Adding Environments","url":"cra/advanced/adding-environments#adding-even-more-environments","content":"Usually, a development and a production environment is enough. In big applications however, you might see the need for more environments. In this example, we will set up a QA environment. To do this, we will simply copy the prod jobs, and create our own for qa instead. .gitlab-ci.yml build:qa: extends: build:dev environment: name: qa rules: - if: '$CI_COMMIT_TAG' image:qa: extends: image:dev variables: IMAGE_TAG: qa rules: - if: '$CI_COMMIT_TAG' needs: [ &quot;build:qa&quot; ] sentry:qa: extends: sentry:dev variables: SENTRY_ENV: qa rules: - if: '$CI_COMMIT_TAG' needs: [ &quot;build:qa&quot; ] .deploy:qa: extends: deploy:dev variables: ROUTE: $CI_PROJECT_NAME-qa.apps.int.intility.no environment: name: qa url: https://$CI_PROJECT_NAME-qa.apps.int.intility.no rules: - if: '$CI_COMMIT_TAG' needs: [ &quot;test&quot;, &quot;image:qa&quot; ]  note We don't override the Sentry release and the image version, to avoid version collisions with the production jobs. This example will, like the production jobs, be triggered by git tags. But it will automatically deploy it to the QA environment, which will be created in the default OpenShift project, unless you've created a custom QA project. "},{"title":"Upgrade to V1 pipeline","type":0,"sectionRef":"#","url":"cra/advanced/upgrade-pipeline","content":"","keywords":""},{"title":"Features​","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#features","content":"Helm chartDocker image pushed to GCRProduction jobsParallelized jobs (pipeline usually takes ~2m) "},{"title":"OpenShift​","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#openshift","content":"When switching to the new pipeline, it will automatically create new resources. At a minimum, you should delete the existing route, as the old one will crash with the new one. However, it's recommended to start blank, by either clearing out or re-creating the existing project, or by creating a new project. "},{"title":"Create Deploy Token​","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#create-deploy-token","content":"The new pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -&gt; Repository -&gt; Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Edit index.ts​","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#edit-indexts","content":"The new pipeline doesn't use a REACT_APP_REDIRECT_URI environment variable in the pipeline, so we need to change auth.redirectUri in our MSAL Config. index.ts const msal = { auth: { // This is the new value of redirectUri redirectUri: window.location.origin } };  "},{"title":"Copy files​","type":1,"pageTitle":"Upgrade to V1 pipeline","url":"cra/advanced/upgrade-pipeline#copy-files","content":"Download the contents of this repository (direct zip download). Copy all files except README.md to your project, and push the changes. The new pipeline should now take effect. Should any problems occur, ask a question in #programming on Slack. "},{"title":"Deploy","type":0,"sectionRef":"#","url":"cra/configuration/deploy","content":"","keywords":""},{"title":"Create project​","type":1,"pageTitle":"Deploy","url":"cra/configuration/deploy#create-project","content":"UICLI Go to our OpenShift instance and log in as Intility Developer. Create a project, the name should be aa-&lt;GITLAB_SLUG&gt;-dev, e.g. aa-my-app-dev. Add a fitting display name and description if you feel like it. "},{"title":"Acquire Token​","type":1,"pageTitle":"Deploy","url":"cra/configuration/deploy#acquire-token","content":"UICLI In your newly created project, switch from Developer to Administrator view in the sidebar. Then go to User Management -&gt; Service Accounts, and click Create Service Account. Change the name field to gitlab-builder. After creating the Service Account, go to User Mangement -&gt; Role Bindings, and click Create Binding. Fill the form with the following values: Role Binding Name: gitlab-builder-edit Role Name: edit Subject: Service Account Subject Name: gitlab-builder After creating the role binding, go back to Service Accounts and go to the gitlab-builder Service Account. At the bottom of page, you'll find a link to a secret named gitlab-builder-token-*, click it. Copy the token field at the bottom of the page, and add it to GitLab CI/CD variables with the key OPENSHIFT_TOKEN. "},{"title":"Environment Variables","type":0,"sectionRef":"#","url":"cra/configuration/environment-variables","content":"Environment Variables Your project can consume variables declared in your environment as if they were declared locally in your JS files. By default you will have NODE_ENV defined for you, and any other environment variables starting with REACT_APP_. By default, .env.development is included, which will be loaded when you run your app locally. For more information, please read the Environment Variables section of the Create React App docs.","keywords":""},{"title":"GitLab Repository","type":0,"sectionRef":"#","url":"cra/configuration/gitlab","content":"","keywords":""},{"title":"Pushing your application​","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#pushing-your-application","content":"Create React App creates a git repository for you locally, but you'll have to add the GitLab repository as a remote. Copy the last three lines of the &quot;Push an existing Git repository&quot; example. They should look something like this: git remote add origin git@gitlab.intility.no:Group/intility-app.git git push -u origin --all git push -u origin --tags  The pipeline will fail initially, but don't worry, we'll set it up correctly in the next steps. "},{"title":"Add GitLab Deploy Token​","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#add-gitlab-deploy-token","content":"The pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -&gt; Repository -&gt; Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Adding CI/CD Variables​","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#adding-cicd-variables","content":"In your project on GitLab, go to Settings -&gt; CI / CD, and expand the Variables section. We don't need to add anything yet, but it's here we'll add tokens and such in the other steps. "},{"title":"Adding Badges​","type":1,"pageTitle":"GitLab Repository","url":"cra/configuration/gitlab#adding-badges","content":"In your project on GitLab, go to Settings -&gt; General, and expand the Badges section. Here you can add badges by giving them a Name, Link and Image URL.  We can go ahead and add a badge for our pipeline with the following values: Name: Pipeline Link: https://gitlab.intility.com/%{project_path} Image URL: https://gitlab.intility.com/%{project_path}/badges/%{default_branch}/pipeline.svg "},{"title":".gitlab-ci.yml Overview","type":0,"sectionRef":"#","url":"cra/configuration/gitlab-ci","content":"","keywords":""},{"title":"build​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#build","content":"The main build job runs npm run build on every push to all branches (except master). It will pass some default environment variables; REACT_APP_SENTRY_DSN will be SENTRY_ENVREACT_APP_SENTRY_ENVIRONMENT will be the git branch nameREACT_APP_SENTRY_RELEASE will be the git commit SHAREACT_APP_REDIRECT_URI will be https://{projectSlug}-{branchName}.openshift-inside.intility.no It also creates an artifact of the build folder that is created, which will be passed onto later jobs. The build:* pipeline does the same as build, except it adds a GitLab environment. The base job uses the GitLab environment name to configure the Sentry environment. "},{"title":"test​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#test","content":"This job simply runs the npm test script. "},{"title":"sonarqube​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#sonarqube","content":"This job uses the SonarScanner CLI to upload the source code to SonarQube. The pipeline will continue if this job fails, but it should be looked into. "},{"title":"image​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#image","content":"The image jobs builds a docker image with the Dockerfile.CI dockerfile, and with the build output from the build jobs. For the image:dev job, it pushes two tags, one with dev, and one with the pipeline id. For the image:prod job, the tags pushed are latest and the tag name. "},{"title":"sentry​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#sentry","content":"The sentry job uses the Sentry CLI to create a new release. The release name will be the git commit SHA. It will connect the release to the current commit, and upload sourcemaps from the build step. Lastly it will associate the release with the right env. "},{"title":"deploy​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"cra/configuration/gitlab-ci#deploy","content":"This is the main job for deploying to OpenShift. It will create an environment in GitLab, with the URL to the running app. It uses Helm to install the app to an OpenShift project. For deploy:dev, the route defaults to https://{projectSlug}-dev.apps.int.intility.no. For deploy:prod, the route defaults to https://{projectSlug}.apps.int.intility.no. "},{"title":"Sentry","type":0,"sectionRef":"#","url":"cra/configuration/sentry","content":"","keywords":""},{"title":"Create Project​","type":1,"pageTitle":"Sentry","url":"cra/configuration/sentry#create-project","content":"Head over to the Create a new Project page in Sentry.  Under platform, select React.  For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. note If you use something other than the GitLab project slug you'll need to modify the SENTRY_PROJECT variable in the top of the .gitlab-ci.yml file. You'll be taken to a Configure React page. In the code example under Connecting the SDK to Sentry, copy the dsn value, and add it to GitLab CI/CD variables with the key SENTRY_DSN. "},{"title":"Acquire Token​","type":1,"pageTitle":"Sentry","url":"cra/configuration/sentry#acquire-token","content":"Go to the Auth Token section of your Account Settings. The needed scopes are project:read, project:releases and org:read. Create it, copy the token and add it to GitLab CI/CD variables with the key SENTRY_AUTH_TOKEN. "},{"title":"SonarQube","type":0,"sectionRef":"#","url":"cra/configuration/sonarqube","content":"","keywords":""},{"title":"Badge​","type":1,"pageTitle":"SonarQube","url":"cra/configuration/sonarqube#badge","content":"Due to intility's sonarqube domain being private, retrieving badge directly from that domain would not work. Therefore you will need to retrieve your project badges from this proxy: sonarqube-badge-proxy.apps.openshift.intility.com/badges/ On your project dashboard in gitlab select the following from the navigation drawer Settings &gt; General &gt; Badges  The proxy works as the following sonarqube-badge-proxy.apps.openshift.intility.com/badges/&lt;your-sonarqube-project-name&gt;/&lt;badge-type&gt; and the following badge types are available: maintainabilitybugscode_smellsduplicationsnclocquality_gatereliabilitysecuritydebtvulnerabilities You will have to replace badge image url using your sonarqube project name and desired badge type e.g sonarqube-badge-proxy.apps.openshift.intility.com/badges/cool-portal/bugs The badge will be rendered in preview if configured correctly as such  note You can also embed badges in readme by using any of the following markdown syntaxes[![Description](proxy-badge-image-url)](url-to-your-project) or![Description](proxy-badge-image-url) "},{"title":"Included Dependencies","type":0,"sectionRef":"#","url":"cra/getting-started/included-dependencies","content":"","keywords":""},{"title":"@intility/bifrost-react​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#intilitybifrost-react","content":"Docs Intility's design system for React. "},{"title":"@intility/react-msal-browser​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#intilityreact-msal-browser","content":"Docs (WIP) @intility/react-* is an abstraction of the 3 auth libraries (adal, msal, msal-browser) by Microsoft for React. "},{"title":"react-router​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#react-router","content":"Docs (v6) Unless you're a dashboard, you need a router, and react-router is the de facto router for React. "},{"title":"@sentry/react and @sentry/tracing​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#sentryreact-and-sentrytracing","content":"Docs Used for error reporting to Sentry. "},{"title":"prettier​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#prettier","content":"Docs Prettier is a code formatter and great tool for unifying formatting when collaborating on a project. The Visual Studio Code Workspace is set up to auto-format on save. "},{"title":"source-map-explorer​","type":1,"pageTitle":"Included Dependencies","url":"cra/getting-started/included-dependencies#source-map-explorer","content":"GitHub See Analyzing the Bundle Size for more information. You can use it by running the following command in your project. npm run analyze  "},{"title":"Installation","type":0,"sectionRef":"#","url":"cra/getting-started/installation","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#prerequisites","content":"Some experience with Reactnode.jsVisual Studio Code with the following plugins Node Extension PackPrettier Set up the Intility NPM registry (optional) "},{"title":"Intility NPM Registry (optional)​","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#intility-npm-registry-optional","content":"The Intility NPM registry (includes access to @intility/* and @fortawesome/* packages) can be configured by running: npm config set registry https://artifactory.int.intility.no/artifactory/api/npm/npm/  For other alternatives, see the guide on developers.intility.no (login required). "},{"title":"Create App​","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#create-app","content":"Select the version you want. By default, TypeScript and an authentication sample is included. default git branch create-react-app automatically creates a git repo. If you want the default branch to be something other than master, you need to set the init.defaultBranch config in git: git config --global init.defaultBranch main You need git version 2.28.0 or higher to use this option. TypeScriptJavaScript npx create-react-app my-app --template &quot;@intility&quot;  Run it in your terminal, and then open the Visual Studio Code Workspace with the following command. code my-app/app.code-workspace  You can now start coding by running the start script. npm start  "},{"title":"Available Scripts​","type":1,"pageTitle":"Installation","url":"cra/getting-started/installation#available-scripts","content":"There are 5 scripts included by default, click on each one for more information. npm startnpm testnpm run buildnpm run ejectnpm run analyze "},{"title":"Project Overview","type":0,"sectionRef":"#","url":"cra/getting-started/project-overview","content":"Project Overview Depending on which template you chose, your project structure will look more or less like the following: ├── public │ ├── favicon.ico │ ├── index.html │ ├── logo192.png │ ├── logo512.png │ ├── manifest.json │ └── robots.txt ├── src │ ├── assets │ │ └── logo.svg │ ├── components │ │ ├── About.tsx │ │ ├── App.css │ │ ├── App.test.tsx │ │ ├── App.tsx │ │ ├── Home.tsx │ │ └── Profile.tsx │ ├── index.tsx │ ├── react-app-env.d.ts │ ├── serviceWorker.ts │ └── setupTests.ts ├── .env.development ├── .eslintrc ├── .gitignore ├── .gitlab-ci.yml ├── .npmrc ├── Dockerfile ├── README.md ├── app.code-workspace ├── package-lock.json ├── package.json └── tsconfig.json ","keywords":""},{"title":"Recommended Packages","type":0,"sectionRef":"#","url":"cra/getting-started/recommended-packages","content":"","keywords":""},{"title":"react-table​","type":1,"pageTitle":"Recommended Packages","url":"cra/getting-started/recommended-packages#react-table","content":"Docs Data modification for tables. Bifrost's Table component is only responsible for the view, and will not do any data modification for you. "},{"title":"swr​","type":1,"pageTitle":"Recommended Packages","url":"cra/getting-started/recommended-packages#swr","content":"Docs Data Fetching library using React hooks. Makes data fetching easy, comes with a cache, Suspense support and more. "},{"title":".NET Templates","type":0,"sectionRef":"#","url":"dotnet/","content":"","keywords":""},{"title":"Recommended reading​","type":1,"pageTitle":".NET Templates","url":"dotnet/#recommended-reading","content":"Tutorial: Create a web API with ASP.NET CoreASP.NET Core fundamentalsIntility Template Source "},{"title":"AAD Authentication","type":0,"sectionRef":"#","url":"cra/configuration/authentication","content":"","keywords":""},{"title":"UI​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#ui","content":"Head over to Azure -&gt; Azure Active Directory -&gt; App registrationswith your Intility Account, and create a new registration.  Select a fitting name for your project, this name will be presented to the user during consent. Under Supported account types, choose either Intility AS only - Single tenant or Any Azure AD directory - Multitenant. This can be changed later, so if you're unsure what to choose, select single tenant. Under Redirect URI, select Single-page application (SPA) from the dropdown, and enter http://localhost:3000. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the clientId field in your index.tsx file. src/index.tsx const msal = { auth: { clientId: &quot;YOUR_CLIENT_ID&quot;, ... }, ... }  Check out Enabling authentication in your app to enable authentication in your app. "},{"title":"Adding reply URLs​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#adding-reply-urls","content":"For each deployment of your app, you'll need to register it. You can do that by going to the Authentication page.  The first we need to add is the URL the deploy step makes in OpenShift: https://{your-project-slug}-dev.apps.int.intility.no  You can also add more later if you create more environments. "},{"title":"CLI​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#cli","content":"Login with the Azure CLI and run the following command (rember to modify the displayName) bashPowerShell az rest -u https://graph.microsoft.com/v1.0/applications -m post -b '{&quot;displayName&quot;:&quot;MyApp&quot;,&quot;spa&quot;:{&quot;redirectUris&quot;:[&quot;http://localhost:3000&quot;]}}'  Copy the appId GUID, and paste it into the clientId field in your index.tsx file. src/index.tsx const msal = { auth: { clientId: &quot;YOUR_CLIENT_ID&quot;, ... }, ... }  Check out Enabling authentication in your app to enable authentication in your app. "},{"title":"Enabling authentication in your app​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#enabling-authentication-in-your-app","content":"Depending on what your applications scope is, there are different ways of enabling authentication. "},{"title":"Forcing auth​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#forcing-auth","content":"This is the simplest way, and can be enabled by adding the forced prop to MsalBrowserProvider. src/index.tsx ReactDOM.render( &lt;Router&gt; &lt;MsalBrowserProvider config={msal} forced&gt; ^^^^^^ &lt;React.StrictMode&gt; &lt;App /&gt; &lt;/React.StrictMode&gt; &lt;/MsalBrowserProvider&gt; &lt;/Router&gt;, document.getElementById(&quot;root&quot;) )  This will force user authentication, and your app won't render unless the user is authenticated. "},{"title":"Conditional auth​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#conditional-auth","content":"If you don't want forced auth, you can implement a simple log-in button by using the useAuth hook from @intility/react-msal-browser. import { Button } from &quot;@intility/bifrost-react&quot;; import { useAuth } from &quot;@intility/react-msal-browser&quot;; const LoginButton = () =&gt; { const { login } = useAuth(); return ( &lt;Button onClick={() =&gt; login()}&gt;Login&lt;/Button&gt; ) };  And elsewhere in your app you can determin the login status by checking the existance of a user with useUser. import { useUser } from &quot;@intility/react-msal-browser&quot;; const MyComponent = () =&gt; { const user = useUser(); return &lt;span&gt;{user !== null ? &quot;Hello friend.&quot; : &quot;Who are you?&quot;}&lt;/span&gt;; };  "},{"title":"Consuming an authenticated API​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#consuming-an-authenticated-api","content":"Consuming an authenticated API is relativly simple. First, you'll need to modify the endpoints section of your MSAL config. The key needs to be the base URL of the API, and the value should be an array of scopes needed to authenticate to the API. src/index.tsx const msal = { endpoints: { &quot;http://localhost:5000&quot;: [&quot;API_SCOPE&quot;], ... }, ... }  The API base URL can also be dynamically applied from an Environment Variables. TypeScriptJavaScript src/index.tsx const msal = { endpoints: { [process.env.REACT_APP_API_URL as string]: [&quot;API_SCOPE&quot;], ... }, ... }  You can now consume an API with authorizedFetch, which uses the endpoints config to detect which token to use in a request. import { useState, useEffect } from &quot;react&quot;; import { authorizedFetch } from &quot;@intility/react-msal-browser&quot;; // This is a simplified example of data fetching in react // Please don't use in production const DataComponent = () =&gt; { const [data, setData] = useState(); const [error, setError] = useState(); useEffect(() =&gt; { authorizedFetch(&quot;my-api/resource&quot;) .then((response) =&gt; response.json()) .then((json) =&gt; setData(json)) .catch((error) =&gt; setError(error)); }, []); return ( &lt;&gt; {!data &amp;&amp; !error &amp;&amp; &lt;p&gt;Loading data...&lt;/p&gt;} {data &amp;&amp; &lt;div&gt;Here is the data: {data}&lt;/div&gt;} {error &amp;&amp; &lt;p className=&quot;error&quot;&gt;Oh No!!! {error.toString()}&lt;/p&gt;} &lt;/&gt; ); };  "},{"title":"SWR / React Query​","type":1,"pageTitle":"AAD Authentication","url":"cra/configuration/authentication#swr--react-query","content":"SWRReact Query Integrating authenticatedFetch with SWR is very simple. All you need to do is specify a fetcher for SWR. TypeScriptJavaScript import useSWR from &quot;swr&quot;; import { authorizedFetch } from &quot;@intility/react-msal-browser&quot;; const authFetcher = (url: string) =&gt; authorizedFetch(url).then((result) =&gt; result.json()); const SwrComponent = () =&gt; { const { data, error } = useSWR(&quot;my-api/resource&quot;, { fetcher: authFetcher }); return ( &lt;&gt; {!data &amp;&amp; !error &amp;&amp; &lt;p&gt;Loading data...&lt;/p&gt;} {data &amp;&amp; &lt;div&gt;Here is the data: {data}&lt;/div&gt;} {error &amp;&amp; &lt;p className=&quot;error&quot;&gt;Oh No!!! {error.toString()}&lt;/p&gt;} &lt;/&gt; ); }; We could take this one step further, and assign the fetcher globally using SWRConfig. TypeScriptJavaScript src/index.tsx import { SWRConfig } from &quot;swr&quot;; import { authorizedFetch } from &quot;@intility/react-msal-browser&quot;; // Note that we apply a base url to every request const swr = { fetcher: (path: string) =&gt; authorizedFetch((process.env.REACT_APP_API_URL as string) + path) .then(response =&gt; response.json()) }; ReactDOM.render( &lt;Router&gt; &lt;MsalBrowserProvider config={msal} forced&gt; &lt;SWRConfig value={swr}&gt; &lt;React.StrictMode&gt; &lt;App /&gt; &lt;/React.StrictMode&gt; &lt;/SWRConfig&gt; &lt;/MsalBrowserProvider&gt; &lt;/Router&gt;, document.getElementById(&quot;root&quot;) ); We can now call useSWR somewhere in our app, and it will by default use the authorized fetcher. import useSWR from &quot;swr&quot;; import { authorizedFetch } from &quot;@intility/react-msal-browser&quot;; const SwrComponent = () =&gt; { // Note that only the resource is requested here // that's because the API base URL is prepended in our fetcher const { data, error } = useSWR(&quot;resource&quot;); return ( &lt;&gt; {!data &amp;&amp; !error &amp;&amp; &lt;p&gt;Loading data...&lt;/p&gt;} {data &amp;&amp; &lt;div&gt;Here is the data: {data}&lt;/div&gt;} {error &amp;&amp; &lt;p className=&quot;error&quot;&gt;Oh No!!! {error.toString()}&lt;/p&gt;} &lt;/&gt; ); }; One great thing about SWR is request deduping and caching. In the following example, there will be three instances of our SwrComponent, but only one request will be made. That would not be the case if we were to use the DataComponent from earlier, where we manually fetched the data. const SomeExample = () =&gt; ( &lt;&gt; &lt;SwrComponent /&gt; &lt;SwrComponent /&gt; &lt;SwrComponent /&gt; &lt;/&gt; ); To read more about SWR, check out the SWR Docs. "},{"title":"Adding Environments","type":0,"sectionRef":"#","url":"dotnet/advanced/adding-environments","content":"","keywords":""},{"title":"Enabling the deploy:prod job​","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#enabling-the-deployprod-job","content":"The provided deploy:prod job is disabled by default. This is to force you to make an active choice on wether you want to use the job or ArgoCD. To enable the job, simply remove the . from the last job. .gitlab-ci.yml - .deploy:prod: + deploy:prod:  "},{"title":"Trigger production jobs​","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#trigger-production-jobs","content":"First, we have to update the appVersion field in our Helm charts Chart.yaml. chart/Chart.yaml appVersion: &quot;x.x.x&quot;  Then, we have to create a git tag and push it git tag x.x.x git push --tags  After all the other jobs has completed, we can manually trigger the deploy:prod job. This is manual by design, and shouldn't be set to automatic. "},{"title":"Separate OpenShift project​","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#separate-openshift-project","content":"You can, and should create a separate OpenShift project to host the production environment. To do this, simply follow the same steps as earlier. This time, name it aa-&lt;project-slug&gt;-prod. When adding the OPENSHIFT_TOKEN to CI/CD variables, select Production as the Environment scope. This variable will then be used instead of the default one in the deploy:prod job.  note If you can't see the Production environment when creating a variable, you need to trigger the production jobs first. The image:prod job will then provision the Production environment, and you can insert the OPENSHIFT_TOKEN before triggering the deploy job. "},{"title":"Adding even more Environments​","type":1,"pageTitle":"Adding Environments","url":"dotnet/advanced/adding-environments#adding-even-more-environments","content":"Usually, a development and a production environment is enough. In big applications however, you might see the need for more environments. In this example, we will set up a QA environment. To do this, we will simply copy the prod jobs, and create our own for qa instead. .gitlab-ci.yml image:qa: extends: image:dev variables: IMAGE_TAG: qa environment: name: QA rules: - if: '$CI_COMMIT_TAG' .deploy:qa: extends: deploy:dev variables: ROUTE: $CI_PROJECT_NAME-qa.apps.int.intility.no environment: name: QA url: https://$CI_PROJECT_NAME-qa.apps.int.intility.no rules: - if: '$CI_COMMIT_TAG' needs: [ &quot;test&quot;, &quot;image:qa&quot; ]  note We don't override the image version to avoid version collisions with the production jobs. This example will, like the production jobs, be triggered by git tags. But it will automatically deploy it to the QA environment, which will be created in the default OpenShift project, unless you've created a custom QA project. "},{"title":"Deploy with ArgoCD","type":0,"sectionRef":"#","url":"dotnet/advanced/deploy-argocd","content":"Deploy with ArgoCD info This page isn't finished yet. If you are comfortable with this subject, feel free to contribute.","keywords":""},{"title":"Features","type":0,"sectionRef":"#","url":"dotnet/features","content":"Features The template is packed with already made decisions, so you don't have to make them. These solutions are a culmination of many years of managing services in production and aims to target established in-house infrastructure. note The templates are updated as the technology and infrastructure changes over time. Helm chartProject debug settingsSwagger configurationDockerfilesREADME templateGitLab CI/CD pipelineAPI VersioningLogging configurationTelemetry instrumentation","keywords":""},{"title":"Azure App Config","type":0,"sectionRef":"#","url":"dotnet/advanced/azure-app-config","content":"","keywords":""},{"title":"Create App Configuration​","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#create-app-configuration","content":"In Azure, create an App Configuration, and go to it.  Copy the endpoint URL, and paste it into the AppConfig field in Properties/launchSettings.json. Properties/launchSettings.json { &quot;profiles&quot;: { &quot;YOUR_PROJECT_NAME&quot;: { ... &quot;environmentVariables&quot;: { &quot;AppConfig&quot;: &quot;MY_APP_CONFIG_ENDPOINT&quot; } } ... } }  The template is set up to use built-in credentials of your development machine. This means that we need to allow your (or more) accounts to access the App Configuration. Go to Access Control (IAM) in the sidebar, then click Add and Add role assignment. Select App Configuration Data Owner, and your own account or a group.  info If you do not have access to add role assignments, you should as someone with the role assignment Owner to do it for you. You can now add Key-values in Configuration manager, and they will be applied to your local development environment. "},{"title":"Create Key Vault​","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#create-key-vault","content":"In Azure, create a Key Vault. When setting up access policies, allow the same group/users you set up in your App Configuration to Get and List the Keys and Secrets. You can now add Key Vault references in the App Configurations Configuration manager, and they will be applied to your local development environment. "},{"title":"Use in development deployment​","type":1,"pageTitle":"Azure App Config","url":"dotnet/advanced/azure-app-config#use-in-development-deployment","content":"Since the config uses your machines credentials to access the App Configuration and Key Vault, it won't automatically work with your development deployment. To set this up, we need to grant your App Registration we set up earlier access to the App Configuration and Key Vault. First, give it the App Configuration Data Reader role in the App Configuration. Then, give it Get and List permissions for Key and Secrets. You also need a client secret for your App Registration, create one in Azure AD -&gt; App Registrations -&gt; Your App Registration -&gt; Certificates &amp; secrets. Set up the following variables in GitLab CI/CD under the Development environment scope AppConfig: The App Configuration endpointAZURE_CLIENT_ID: The App Registration client idAZURE_CLIENT_SECRET: The client secret you just madeAZURE_TENANT_ID: The tenant id of your App Registration (check in your App Registrations Overview) Lastly, we need to configure the development deployment to use these variables. We do this by passing the variables we just set up to Helm: .gitlab-ci.yml deploy:dev: ... script: - oc login $OPENSHIFT_SERVER --token=$OPENSHIFT_TOKEN - helm upgrade --install $CI_ENVIRONMENT_SLUG ./Company.WebApplication1/chart --set nameOverride=$CI_PROJECT_NAME --set image.repository=$CI_REGISTRY_IMAGE --set image.tag=$IMAGE_TAG --set registry.url=$CI_REGISTRY --set registry.user=$CI_DEPLOY_USER --set registry.password=$CI_DEPLOY_PASSWORD --set route=$ROUTE --set replicaCount=2 --set secrets.Sentry__Dsn=$SENTRY_DSN --set config.ASPNETCORE_ENVIRONMENT=$CI_ENVIRONMENT_NAME --set config.AppConfig=$AppConfig --set config.AZURE_CLIENT_ID=$AZURE_CLIENT_ID --set config.AZURE_CLIENT_SECRET=$AZURE_CLIENT_SECRET --set config.AZURE_TENANT_ID=$AZURE_TENANT_ID  The development environment will now be able to connect and use the configuration from the App Registration and Key vault. note Don't worry, even though the deploy:prod job extends the deploy:dev job, the variables won't be set, since we only scoped them to the Development environment. "},{"title":"Installation","type":0,"sectionRef":"#","url":"dotnet/installation","content":"","keywords":""},{"title":"Bootstrap new project using the template​","type":1,"pageTitle":"Installation","url":"dotnet/installation#bootstrap-new-project-using-the-template","content":"You can either use the dotnet new command or Visual Studio -&gt; New Project wizard to create a new project based on the templates. dotnet CLIVisual Studio note If you only want to create a project, and skip creating a solution, use the --project argument. # create project with solution dotnet new iwebapi -o MyService # create project only dotnet new iwebapi -o MyService --project # run project cd MyService dotnet run --project MyService/MyService.csproj  "},{"title":"Updating the template​","type":1,"pageTitle":"Installation","url":"dotnet/installation#updating-the-template","content":"danger Earlier versions of the CLI had troubles updating packages due to long-lived caches. You need to be on version 5.0.301 or higher for updating to work properly. note This will not update already bootstrapped projects. Check for updates by running dotnet new --update-check  If there are any updates available, update with dotnet new --update-apply  or dotnet new --install Intility.Templates  "},{"title":"AAD Authorization","type":0,"sectionRef":"#","url":"dotnet/setup/authorization","content":"","keywords":""},{"title":"API​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#api","content":"Head over to Azure -&gt; Azure Active Directory -&gt; App registrationswith your Intility Account, and create a new registration.  Select a fitting name for your project, this name will be presented to the user during consent. Under Supported account types, choose either Intility AS only - Single tenant or Any Azure AD directory - Multitenant. This can be changed later, so if you're unsure what to choose, select single tenant. Leave the Redirect URI section blank. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the AzureAd:ClientId field in your appsettings.json file. appsettings.json { &quot;AzureAd&quot;: { &quot;ClientId&quot;: &quot;YOUR_CLIENT_ID&quot;, ... }, ... }  info If you chose Multitenant, set the AzureAd:TenantId field to common. "},{"title":"Add an application scope​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#add-an-application-scope","content":"Go to Expose an API in your app registration, and add a scope. You will be prompted to set an Application ID URI. Use the suggested one, and hit Save and continue.  Add a scope named user_impersonation, that can be consented by Admins and users. You can use the following descriptions: Access API as user Allows the app to access the API as the user. Access API as you Allows the app to acces the API as you.  "},{"title":"Swagger​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#swagger","content":"In addition to creating an App Registration for the API itself, we need to make one for the Swagger client too. Again head over to Azure -&gt; Azure Active Directory -&gt; App registrations.  Use the same name appended with Swagger. Under Redirect URI, select Single-page application (SPA) and enter http://localhost:5000/oauth2-redirect.html. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the AzureAd:ClientId field in your appsettings.json file. appsettings.json { &quot;Swagger&quot;: { &quot;ClientId&quot;: &quot;YOUR_SWAGGER_CLIENT_ID&quot;, ... }, ... }  "},{"title":"Adding reply URLs​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#adding-reply-urls","content":"For each deployment of your app, you'll need to register it. You can do that by going to the Authentication page.  The first reply URLs we need to add are the localhost https URL, and the OpenShift deploy URL: https://localhost:5001/oauth2-redirect.html https://{your-project-slug}-dev.apps.int.intility.no/oauth2-redirect.html  You can also add more later if you create more environments. "},{"title":"Access API​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#access-api","content":"To allow Swagger to talk to the API, you need to add API permissions to the Swagger app registration. Go to API permissions, and hit Add a permission. Under My APIs, find your API, select the scope(s) and press Add permissions.  "},{"title":"Guest users​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#guest-users","content":"The template comes with an authorization policy that denies guest users in Azure AD from accessing the API. This policy is enabled when the application is not set up as multitenant. If you want guest users to access your single tenant API, simply remove the lines applying the policy. Startup.cs services.AddAuthorization(options =&gt; { var tenantId = Configuration[&quot;AzureAd:TenantId&quot;]; if (tenantId != &quot;common&quot; &amp;&amp; tenantId != &quot;organizations&quot;) { options.AddPolicy(&quot;NoGuests&quot;, policy =&gt; policy.RequireClaim( ClaimConstants.TenantId, tenantId)); } });  "},{"title":"User Assignment in Enterprise Application​","type":1,"pageTitle":"AAD Authorization","url":"dotnet/setup/authorization#user-assignment-in-enterprise-application","content":"The policy successfully denies guest users access to the API. However, it's not very user friendly, since the user won't know their denied until they call the API. Using user assignment in Enterprise Applications, we can deny users during authentication. Go to Azure AD -&gt; Enterprise Applications and find your application (you can search by Client ID). Under Properties, enable User assignment required? and save.  Then, go to Users and groups, and add user/group. Find users or a fitting group and assign it to the role Default Access. note Groups you select should have all users as direct members of the group. Nested groups does not work with Enterprise Applications. This should be done for all app registrations (API, Swagger and frontend). For more information, check out our internal docs. "},{"title":"Deploy","type":0,"sectionRef":"#","url":"dotnet/setup/deploy","content":"","keywords":""},{"title":"Create project​","type":1,"pageTitle":"Deploy","url":"dotnet/setup/deploy#create-project","content":"UICLI Go to our OpenShift instance and log in as Intility Developer. Create a project, the name should be aa-&lt;GITLAB_SLUG&gt;-dev, e.g. aa-my-api-dev. Add a fitting display name and description if you feel like it. "},{"title":"Acquire Token​","type":1,"pageTitle":"Deploy","url":"dotnet/setup/deploy#acquire-token","content":"UICLI In your newly created project, switch from Developer to Administrator view in the sidebar. Then go to User Management -&gt; Service Accounts, and click Create Service Account. Change the name field to gitlab-builder. After creating the Service Account, go to User Mangement -&gt; Role Bindings, and click Create Binding. Fill the form with the following values: Role Binding Name: gitlab-builder-edit Role Name: edit Subject: Service Account Subject Name: gitlab-builder After creating the role binding, go back to Service Accounts and go to the gitlab-builder Service Account. At the bottom of page, you'll find a link to a secret named gitlab-builder-token-*, click it. Copy the token field at the bottom of the page, and add it to GitLab CI/CD variables with the key OPENSHIFT_TOKEN. "},{"title":".gitlab-ci.yml Overview","type":0,"sectionRef":"#","url":"dotnet/setup/gitlab-ci","content":"","keywords":""},{"title":"Jobs​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#jobs","content":"info The image:dev and image:prod jobs is set to run on pushes to master or main branch. info The image:prod and deploy:prod jobs are set to run when tags are pushed to the repository. "},{"title":"build​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#build","content":"Builds the project, and creates a build artifact for later stages to use. "},{"title":"test​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#test","content":"Runs the tests in the project. Runs parallel with build. "},{"title":"analyze​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#analyze","content":"Analyzes the project with sonar-scanner. Runs parallel with build. "},{"title":"image​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#image","content":"Uses the artifact from the build step, and creates and publish a docker image with the Dockerfile.CI file using kaniko. Runs once the build job has finished. "},{"title":"deploy​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#deploy","content":"Deploys the application to OpenShift using the Helm chart. Runs once the image and test jobs have succeeded. "},{"title":"Debugging the pipeline​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#debugging-the-pipeline","content":""},{"title":"Paths​","type":1,"pageTitle":".gitlab-ci.yml Overview","url":"dotnet/setup/gitlab-ci#paths","content":"Ensure the paths used are right. In the build step, we need to ensure that we are working from the right folder. .gitlab-ci.yml build: script: - dotnet restore - cd YOUR_PROJECT_FOLDER # cd to the project folder - dotnet publish -o build artifacts: paths: - YOUR_PROJECT_FOLDER/build # the build in the project folder  In image:dev, we need the context parameter to be set to the project folder. .gitlab-ci.yml image:dev: script: - echo ... - /kaniko/executor --context $CI_PROJECT_DIR/YOUR_PROJECT_FOLDER # sets the context to your project folder  In deploy:dev, ensure the path to the chart is right. .gitlab-ci.yml deploy:dev: script: - oc login $OPENSHIFT_SERVER --token=$OPENSHIFT_TOKEN - helm upgrade --install $CI_ENVIRONMENT_SLUG ./YOUR_PROJECT_FOLDER/chart # path to chart  "},{"title":"GitLab Repository","type":0,"sectionRef":"#","url":"dotnet/setup/gitlab","content":"","keywords":""},{"title":"Pushing your application​","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#pushing-your-application","content":"Creating a dotnet project from a template does not automatically create a git repository for you, so we have to make one ourselves. Follow the &quot;Push an existing folder&quot; example, which should look something like this: git init git remote add origin git@gitlab.intility.no:Group/intility-api.git git add . git commit -m &quot;Initial commit&quot; git push -u origin master  The pipeline will fail initially, but don't worry, we'll set it up correctly in the next steps. "},{"title":"Add GitLab Deploy Token​","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#add-gitlab-deploy-token","content":"The pipeline publishes a docker image to GitLab Container Registry, and uses Helm to create resources in OpenShift. As a result, OpenShift needs access to pull images from GCR. In your GitLab repository, go to Settings -&gt; Repository -&gt; Deploy tokens. Create a new token named gitlab-deploy-token, and give it the read_registry scope.  That's all you have to do in the repository. The token is exposed as a variable in the pipeline, and helm applies it as a pull secret in OpenShift. Read more about GitLab deploy tokens here. "},{"title":"Adding CI/CD Variables​","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#adding-cicd-variables","content":"In your project on GitLab, go to Settings -&gt; CI / CD, and expand the Variables section. We don't need to add anything yet, but it's here we'll add tokens and such in the other steps. "},{"title":"Adding Badges​","type":1,"pageTitle":"GitLab Repository","url":"dotnet/setup/gitlab#adding-badges","content":"In your project on GitLab, go to Settings -&gt; General, and expand the Badges section. Here you can add badges by giving them a Name, Link and Image URL.  We can go ahead and add a badge for our pipeline with the following values: Name: Pipeline Link: https://gitlab.intility.com/%{project_path} Image URL: https://gitlab.intility.com/%{project_path}/badges/%{default_branch}/pipeline.svg "},{"title":"Sentry","type":0,"sectionRef":"#","url":"dotnet/setup/sentry","content":"","keywords":""},{"title":"Create Project​","type":1,"pageTitle":"Sentry","url":"dotnet/setup/sentry#create-project","content":"Head over to the Create a new Project page in Sentry.  Under platform, select .NET.  For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. You'll be taken to a Configure .NET page. In the code examples, copy the DSN value passed to Sentry.Init, and add it to GitLab CI/CD variables with the key SENTRY_DSN. "},{"title":"SonarQube","type":0,"sectionRef":"#","url":"dotnet/setup/sonarqube","content":"","keywords":""},{"title":"Badge​","type":1,"pageTitle":"SonarQube","url":"dotnet/setup/sonarqube#badge","content":"Due to intility's sonarqube domain being private, retrieving badge directly from that domain would not work. Therefore you will need to retrieve your project badges from this proxy: sonarqube-badge-proxy.apps.openshift.intility.com/badges/ On your project dashboard in gitlab select the following from the navigation drawer Settings &gt; General &gt; Badges  The proxy works as the following sonarqube-badge-proxy.apps.openshift.intility.com/badges/&lt;your-sonarqube-project-name&gt;/&lt;badge-type&gt; and the following badge types are available: quality_gatemaintainabilitybugscode_smellsduplicationsnclocreliabilitysecuritydebtvulnerabilities You will have to replace badge image url using your sonarqube project name and desired badge type e.g sonarqube-badge-proxy.apps.openshift.intility.com/badges/cool-portal/bugs The badge will be rendered in preview if configured correctly as such:  note You can also embed badges in readme by using any of the following markdown syntaxes[![Description](proxy-badge-image-url)](url-to-your-project) or![Description](proxy-badge-image-url) "},{"title":"Configuration","type":0,"sectionRef":"#","url":"dotnet/topics/configuration","content":"","keywords":""},{"title":"Azure AD Authentication​","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#azure-ad-authentication","content":" ⚠️ TODO: App RegistrationsThe relationship between SPA App, Resource App How to configure them properly  { &quot;AzureAd&quot;: { // azure identity platform instance (should not be changed) &quot;Instance&quot;: &quot;https://login.microsoftonline.com/&quot;, // primary domain of your tenant &quot;Domain&quot;: &quot;intility.onmicrosoft.com&quot;, // tenant / directory id &quot;TenantId&quot;: &quot;9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6&quot;, // client id of your azure appReg &quot;ClientId&quot;: &quot;11111111-1111-1111-11111111111111111&quot; } }  "},{"title":"Swagger​","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#swagger","content":" ⚠️ TODO: Swagger authenticationSwagger versioning relationship with API versioning  { &quot;Swagger&quot;: { // Name of the service in Swagger definition and UI &quot;AppName&quot;: &quot;Company.WebApplication1 Swagger&quot;, // The client id to authenticate with. This should be an // app registration with delegated permission to the API registration &quot;ClientId&quot;: &quot;22222222-2222-2222-22222222222222222&quot; } }  "},{"title":"Logging​","type":1,"pageTitle":"Configuration","url":"dotnet/topics/configuration#logging","content":"Logging is provided by the external package Intility.Logging.AspNetCore in conjunction with logger sink extensions. The goal with the external package is to enable continuous development of logging support centrally, as the infrastructure changes over time, alleviating you of this burden. note For more information about logging configurations visit the Logging section. { &quot;Serilog&quot;: { &quot;MinimumLevel&quot;: { &quot;Default&quot;: &quot;Information&quot;, &quot;Override&quot;: { &quot;System&quot;: &quot;Warning&quot;, &quot;Microsoft&quot;: &quot;Warning&quot;, &quot;Microsoft.Hosting.Lifetime&quot;: &quot;Information&quot; } }, &quot;Properties&quot;: { &quot;Application&quot;: &quot;MyService&quot; } } }  "},{"title":"Logging","type":0,"sectionRef":"#","url":"dotnet/topics/logging","content":"","keywords":""},{"title":"Creating Logs​","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#creating-logs","content":"To create logs, use an ILogger&lt;TCategoryName&gt; object from dependency injection (DI). Let's take a look at the following example: [ApiController] [Route(&quot;[controller]&quot;)] public class CustomerController : ControllerBase { private readonly ILogger _logger; public CustomerController(ILogger&lt;CustomerController&gt; logger) { _logger = logger; } [HttpPost] public ActionResult&lt;Customer&gt; Create(CreateCustomerDTO createCustomerDto) { _logger.LogInformation( &quot;Customer endpoint called at {Timestamp}&quot;, DateTime.UtcNow.ToLongTimeString()); //... } }  The DI system creates a logger and injects it into the controller. In this example the logger uses a log category of the fully qualified name of the type CustomerController. The log category is a string that is associated with each log entry and makes it easier to troubleshoot large quantities of logs. One can also observe that we are logging with the LogInformation extension method. Information is one of several log levels available - ordered by criticality: Debug:General debug information used in development environments with high verbosity Trace: Events related to code performance and infrastructure Information:Low criticality application information Warning:Medium criticality information and recoverable exceptions Error:Hight criticality information about non-recoverable exceptions and faulty states Critical:Severely critical messages related to crashes where the process cannot recover "},{"title":"Structured Logging​","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#structured-logging","content":"In addition to the message we are also taking advantage of the structured logging capabilities baked into the logger. This means that we can inject arbitrary metadata surrounding our log events to make troubleshooting even easier. In the example above we are injecting the current timestamp into a property called Timestamp. Further more, we can also pass inn serializable objects like reference types or records. Serilog will automatically serialize the properties for you. Note that you have to prefix your property name with an @ symbol for this serialization to take place var albert = new { Name = &quot;Albert Aaberg&quot; }; logger.LogWarning(&quot;Could not find any friends of {@User}&quot;, albert);  "},{"title":"Scoping​","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#scoping","content":"It is sometimes useful to attach common metadata to a series of log events in your application instead of placing them separately in the log message itself. var props = new Dictionary&lt;string,object&gt;() { { &quot;Name&quot;, &quot;Albert Aaberg&quot;}, { &quot;Friends&quot;, 0 } }; using(logger.BeginScope(props)) { logger.LogInformation(&quot;Every log message inside this scope has metadata attached&quot;); } logger.LogInformation(&quot;The props has left the building&quot;);   "},{"title":"Configure Logging​","type":1,"pageTitle":"Logging","url":"dotnet/topics/logging#configure-logging","content":"With the serilog configuration it is possible to override the verbosity of the logs for each namespace separately. Very useful if you need to up the verbosity of your own code without clutter. NB: The overrides should always be stricter than the default MinimumLevel. There is also a very handy Properties section that lets you define custom metadata surrounding all your log events. The application host can also inject additional context to your logs through environment variables through the same configuration infrastructure. Configuration is usually provided by the Serilog section of the appsettings.{Environment}.json files. The following appsettings.json file is generated by the template. { &quot;Serilog&quot;: { &quot;MinimumLevel&quot;: { &quot;Default&quot;: &quot;Information&quot;, &quot;Override&quot;: { &quot;System&quot;: &quot;Warning&quot;, &quot;Microsoft&quot;: &quot;Warning&quot;, &quot;Microsoft.Hosting.Lifetime&quot;: &quot;Error&quot; } }, &quot;Properties&quot;: { &quot;Application&quot;: &quot;MyService&quot; } }  You can read more about logging at the official .NET Core documentation and by visiting the Serilog documentation for a complete overview of the configuration capabilities. Logging in .NET Core and ASP.NET Core~55 minutes to read "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"express/GettingStarted","content":"","keywords":""},{"title":"Scaffolding the project​","type":1,"pageTitle":"Getting Started","url":"express/GettingStarted#scaffolding-the-project","content":"npm install -g yo # Install Yeoman CLI npm install -g @intility/generator-express # Install Intility Express project generator mkdir &lt;projectName&gt; &amp;&amp; cd &lt;projectName&gt; # Create a new project folder yo @intility/express # Use Yeoman CLI to run the template generator  info During the project scaffolding you will be asked the following questions: Project name and description These values will be injected into the package.json file and will also ble shown in the Swagger documentation.Default project name will be the name of the folder. Do you want to include demo endpoints in the generated project? This is recommended for first time setup or if you are not familiar with Express API architecture These files provides example CRUD operations with a router-, controller- and service-layer Automatic Git initialization and push to given Git repository. Please create a GitLab project, and get the SSH key. NOTE: Unselect the: 'Initialize repository with a README' under 'Project Configuration' info During dependency installation stage of the project initialization it may be shown an error message for missing Python or C++ compiler. Please ignore this error message "},{"title":"Running the project​","type":1,"pageTitle":"Getting Started","url":"express/GettingStarted#running-the-project","content":"After the project have successfully been scaffolded you are ready to start the server. code . # Open project in VS Code npm run start # Start the project  After starting the project an Missing Environment variable: INTILITY_TENANT_ID Error will ber thrown. To fix this, create an .env file by using the template provided in .env.template and setup authentication by following this (Guide) For an more comprehensive overview of all included Npm Scripts, have a look at this page: NPM Scripts "},{"title":"What should I do next?​","type":1,"pageTitle":"Getting Started","url":"express/GettingStarted#what-should-i-do-next","content":"Generate SonarQube Token (Guide) Add the badge (Guide) Add Code coverage regexp to Gitlab to make the badge work. Navigate to: GitLab Repository -&gt; Settings -&gt; CI/CD -&gt; General pipelines -&gt; Test coverage parsingEnter: All files[^|]*\\|[^|]*\\s+([\\d\\.]+) Create an .env file by using the template provided in .env.template Setup authentication (Guide) Setup your OpenShift environment: Create a project on OpenShift (Guide)Create a Deploy Token for your GitLab Repository (Guide) "},{"title":"Express TypeScript","type":0,"sectionRef":"#","url":"express/","content":"","keywords":""},{"title":"Getting started using this template​","type":1,"pageTitle":"Express TypeScript","url":"express/#getting-started-using-this-template","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Express TypeScript","url":"express/#prerequisites","content":"Some experience with JavaScriptNode.js version &gt; 14 installed. You can find the installer here: Download | Node.jsDocker and Docker Compose, optional.Git You'll also need to install Yeoman to install this template. Yeoman Yeoman is a tool for scaffolding tool for generating new applications. Yeoman combines the powerful build configuration tool Gulp with npm for handling dependencies. "},{"title":"Goals​","type":1,"pageTitle":"Express TypeScript","url":"express/#goals","content":"Provide an ready to develop TypeScript configuration setup.Ready configured EsLint with the most common rules.Ready configured test environment using Jest Async error handling.Logging to Azure Application Insights.Integrate common security measures: CORSAuthentication and authorization using Passport as well as custom common token rules. Including Role-based access control, guest user validation and checking user tenant. "},{"title":"Questions and Contributions​","type":1,"pageTitle":"Express TypeScript","url":"express/#questions-and-contributions","content":"While this setup might work, it is nor perfect or complete! Maybe I'm missing some key part? or you have a suggestion on something that can be done different or in another way? If you have any questions or suggestions, please send an merge request on the GitLab project located HERE or message me at Microsoft Teams "},{"title":"NPM Scripts","type":0,"sectionRef":"#","url":"express/NpmScripts","content":"NPM Scripts { &quot;scripts&quot;: { // Clean up folders generated by `compile` command &quot;clean&quot;: &quot;rimraf build&quot;, // Compile the TypeScript code into JavaScript code that can be run by Node.JS. &quot;tsc:compile&quot;: &quot;tsc&quot;, // Coly static files folder into build folder &quot;copy:static&quot;: &quot;cp -r src/static/ build/src/&quot;, // Compile and copy static files &quot;compile&quot;: &quot;run-s tsc:compile copy:static&quot;, // Run ESLint code quality check &quot;lint&quot;: &quot;eslint src/**/*.ts&quot;, // Fix issues reported by ESLint &quot;lint:fix&quot;: &quot;eslint src/**/*.ts --fix&quot;, // Start the development server by using nodemon for hot reloading &quot;start&quot;: &quot;nodemon&quot;, // Run tests &quot;test&quot;: &quot;jest&quot;, // Run tests and watch for changes &quot;test:watch&quot;: &quot;jest --watch&quot;, // Run tests and emit code coverage reports &quot;test:cov&quot;: &quot;jest --coverage&quot; }, } ","keywords":""},{"title":"Azure Application Registrations","type":0,"sectionRef":"#","url":"express/Setup/ApplicationRegistrations","content":"","keywords":""},{"title":"UI​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#ui","content":"Head over to Azure -&gt; Azure Active Directory -&gt; App registrationswith your Intility Account, and create a new registration.  Select a fitting name for your project, this name will be presented to the user during consent. Under Supported account types, choose either Intility AS only - Single tenant or Any Azure AD directory - Multitenant. This can be changed later, so if you're unsure what to choose, select single tenant. Leave the Redirect URI section blank. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID and Directory (Tenant) ID and paste it into the APP_CLIENT_ID and INTILITY_TENANT_ID field in your .env file: .env INTILITY_TENANT_ID= APP_CLIENT_ID=  info If you chose Multitenant, set the INTILITY_TENANT_ID field to common. "},{"title":"Add an application scope​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#add-an-application-scope","content":"Go to Expose an API in your app registration, and add a scope. You will be prompted to set an Application ID URI. Use the suggested one, and hit Save and continue.  Add a scope named user_impersonation, that can be consented by Admins and users. You can use the following descriptions: Access API as user Allows the app to access the API as the user. Access API as you Allows the app to access the API as you.  "},{"title":"Swagger​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#swagger","content":"In addition to creating an App Registration for the API itself, we need to make one for the Swagger client too. Again head over to Azure -&gt; Azure Active Directory -&gt; App registrations.  Use the same name appended with Swagger. Under Redirect URI, select Single-page application (SPA) and enter http://localhost:5000/oauth2-redirect.html. Hit the register button, and you will be taken to an overview of your newly created registration.  Copy the Application (Client) ID GUID, and paste it into the SWAGGER_APP_CLIENT_ID field in your appsettings.json file. .env SWAGGER_APP_CLIENT_ID=  "},{"title":"Adding reply URLs​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#adding-reply-urls","content":"For each deployment of your app, you'll need to register it. You can do that by going to the Authentication page.  The first reply URLs we need to add are the localhost https URL, and the OpenShift deploy URL: http://localhost:4000/oauth2-redirect.html  You can also add more later if you create more environments. "},{"title":"Access API​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#access-api","content":"To allow Swagger to talk to the API, you need to add API permissions to the Swagger app registration. Go to API permissions, and hit Add a permission. Under My APIs, find your API, select the scope(s) and press Add permissions.  "},{"title":"Implementing auth to endpoints.​","type":1,"pageTitle":"Azure Application Registrations","url":"express/Setup/ApplicationRegistrations#implementing-auth-to-endpoints","content":"For more info about how to authorize users on your endpoints, head over to this guide: Security | Authorization. "},{"title":"Security - Authorization","type":0,"sectionRef":"#","url":"express/Setup/Security/Authorization","content":"","keywords":""},{"title":"Authentication​","type":1,"pageTitle":"Security - Authorization","url":"express/Setup/Security/Authorization#authentication","content":"In some cases you only want to authenticate your user. to do this, add the authenticate middleware to your routes: router.get(&quot;/&quot;, authenticate, controller.getByOid); router.post(&quot;/&quot;, authenticate, controller.createNewUser);  "},{"title":"Authorization​","type":1,"pageTitle":"Security - Authorization","url":"express/Setup/Security/Authorization#authorization","content":"Authorization is mostly done by inspecting the authenticated token and conditionally allow or decline users from accessing certain endpoints. We are checking for values like: tid or tenantId: The authenticated users origin tenant ID.acct or account: Users account status in tenant.roles: List of roles for this application. "},{"title":"User values in token​","type":1,"pageTitle":"Security - Authorization","url":"express/Setup/Security/Authorization#user-values-in-token","content":"It's equally important to ensure that the API only responds with data or allow actions which corresponds to the authorized user's role. An example can be that you've created a multi-tenant API with a common endpoint for fetching a company's mobile subscriptions. It's then essential to actually force that multi-tenancy in your code. To accomplish this we can take a look at some of the claims provided in the token. We use token claims because they contain information about the user which are provided by a trusted resource (AD/Azure AD). upn, User principal name: this is the same as primary email for Intility users and can be used to identify the user.tid, Tenant ID: this can either be used alone if you have this filed in your dataset, or with ROT-OData API to lookup the company GUID or company code. This is especially important for multi tenant applications. "},{"title":"Roles​","type":1,"pageTitle":"Security - Authorization","url":"express/Setup/Security/Authorization#roles","content":"An official guide on how to configure your application registration to use app roles and receiving them in the token can be found in this guide: How to: Add app roles to your application and receive them in the token Validating and checking the token is done by creating an Express middleware that validating the claims provided by the token. Passport saves the decoded token fields in the request object. To implement authorization on your endpoints, add the authorize middleware to your routes and provide a set of roles as the parameter router.get(&quot;/&quot;, authenticate, authorize(['Api.Read']), controller.getByOid); router.post(&quot;/&quot;, authenticate, authorize(['Api.Read', 'Api.Write']), controller.createNewUser);  BONUS: ROT OData API query​ The following query can be used to get a Company's GUID and/or code (e.g. AA) by the tid claim provided in the token by using the ROT OData API. You can find the Official API documentation here: open-rot-api &lt;ROT_BASE_URL&gt;/ROTDirectoryAzureTenant?$filter=AzureTenantGUID eq &lt;TENANT_ID_FROM_TOKEN&gt;&amp;$select=Directory, AzureTenantGUID&amp;$expand=Directory($select=Name, CompanyGUID) "},{"title":"Project Structure","type":0,"sectionRef":"#","url":"express/ProjectStructure","content":"Project Structure Depending on which template you chose, your project structure will look more or less like the following: │ .env.template # Template file for .env values │ .eslintrc.json # ESLint configuration file │ .gitignore # Files/Directories that Git should ignore │ .gitlab-ci.yml # GitLab CI Pipeline configuration │ docker-compose.yml # Docker Compose file │ Dockerfile # Docker image file │ jest.config.ts # Jest configuration file │ nodemon.json # Nodemon configuration file │ package-lock.json # Installed NPM dependencies │ package.json # NPM configuration and dependency file │ README.md # Project README │ tsconfig.json # TypeScript configuration file │ ├───.vscode # VS Code configuration files │ launch.json │ settings.json │ ├───assets # Media/assets used across the project │ mongodb-docker-compose.yml │ ├───chart # Helm charts for OpenShift deployment │ │ Chart.yaml │ │ values.yaml │ │ │ └───templates │ config.yaml │ deployment.yaml │ pull-secret.yaml │ route.yaml │ secrets.yaml │ service.yaml │ _helpers.tpl │ ├───src # Project source code │ │ baseRouter.ts # Base Express router exposed on `/api/v1` path │ │ index.ts # Main entrypoint │ │ │ ├───api │ │ ├───health # Endpoints for health checking │ │ │ │ health.controller.ts │ │ │ │ health.router.ts │ │ │ │ │ │ │ └───dto # Interfaces/typings for Health response DTOs │ │ │ health.dto.ts │ │ │ │ │ └───users # (Optional) Example CRUD User entity │ │ │ user.router.ts # Express sub router for User entity │ │ │ user.controller.ts # Controller class for handling requests │ │ │ user.service.ts # Service for handling business logic │ │ │ │ │ ├───dto # DTOs (Data Transfer Object) │ │ │ create-user.dto.ts # DTO for creating a new User │ │ │ update-user.dto.ts # DTO for updating an existing user │ │ │ user.dto.ts # DTO for returning a user │ │ │ │ │ └───entities # Entities used in databases │ │ user.entity.ts # User entity │ │ │ ├───config # Common server configuration │ │ app.ts │ │ appInsights.ts │ │ LoggerService.ts │ │ mongoose.ts │ │ swagger.ts │ │ │ ├───interfaces # Common interfaces/types used across the projects │ │ authorize.ts │ │ token.ts │ │ │ ├───middlewares # Express middlewares │ │ errorHandler.ts │ │ passport.ts │ │ │ ├───static # Static files to be exposed as endpoints │ │ oauth2-redirect.html │ │ oAuthHandler.js │ │ README.md │ │ │ └───utils # Common utilities │ ├───mocks # Mocking values │ │ passport.ts │ │ │ └───test # Utilities for testing of the application │ expressRouterUtils.ts │ └───__tests__ # Folder for test-files. │ baseRouter.test.ts │ ├───api # Folder for testing API resources │ ├───health # Tests for health services │ │ health.controller.test.ts │ │ health.router.test.ts │ │ │ └───users # (Optional) Tests for example User entity │ users.controller.test.ts │ users.router.test.ts │ users.service.test.ts │ └───middlewares # Folder for testing middlewares errorHandler.test.ts passport.test.ts ","keywords":""},{"title":"Security - CORS","type":0,"sectionRef":"#","url":"express/Setup/Security/CORS","content":"Security - CORS Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served. By default only services (clients and servers) hosted on the same server can access each other. To allow other services to access your server you have to whitelist them using HTTP Headers manually or by using dependencies that helps you out with this. info CORS policy is enforced by web browsers, but needs to be configured by the APIs. We will use the NPM package cors to configure CORS rules. src/config/app.ts app.use(cors({ origin: [ 'my-super-awesome-frontend.azurewebsites.net', 'my-cool-background-process.azurewebsites.net' ] })); There are many ways to configure CORS rules, for a more in-depth configuration documentation. See the dependency's README","keywords":""},{"title":"API - Sentry","type":0,"sectionRef":"#","url":"express/Setup/Sentry","content":"API - Sentry Head over to the Create a new Project page in Sentry. Under platform, select React. For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. You'll be taken to a Configure Express page. In the code example under Connecting the SDK to Sentry, copy the dsn value and add this to a new secret in your .env file. SENTRY_DSN=XXXXX ","keywords":""},{"title":"Azure Event Grid","type":0,"sectionRef":"#","url":"express/Topics/Azure-Event-Grid","content":"Azure Event Grid In this chapter i'll go publishing events to Intility's EventGrid called Metro. Guides to what Metro is and how is works can be found here: Official Intility documentation on Metro TODO write text and create examples JS sample on publishing eventsJS sample on subscribing to events","keywords":""},{"title":"Azure Application Insights","type":0,"sectionRef":"#","url":"express/Topics/Application-Insights","content":"Azure Application Insights NOTE: This assumes that you have an Azure Application Insights resource created for your project applicationinsights - Microsoft Application Insights module for Node.js. npm i applicationinsights Find your instrumentation key in Azure Portal here: And add this to a new secret in your .env file: APPINSIGHTS_INSTRUMENTATIONKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx More on how to configure Application insights can be found here: Monitor your Node.js services and apps with Application Insights","keywords":""},{"title":"Database","type":0,"sectionRef":"#","url":"express/Topics/Database","content":"","keywords":""},{"title":"Mongoose and MongoDB​","type":1,"pageTitle":"Database","url":"express/Topics/Database#mongoose-and-mongodb","content":"Mongoose is an ORM (Object–relational mapping) for MongoDB that helps you with the more advanced interaction with MongoDB. Some of the features mongoose helps you with are: Population of foreign keysSimpler queryingStatic modellingData validation... much more! Included in this project is an docker-compose.yml file to run an local MongoDB database as an Docker Image, as well as an connection string in index.ts configured to connect to this database. This file is located here: assets/docker-compose.yml NOTE: This is meant for local development purposes only! Please move the connection string to .env and use an external MongoDB database or an Azure CosmosDB with MongoDB API "},{"title":"Other Databases​","type":1,"pageTitle":"Database","url":"express/Topics/Database#other-databases","content":"For other SQL-like databases it is recommended to utilize an ORM (Object–relational mapping) framework. The most popular ones are: TypeORM - Main DocumentationSequelize - Main Documentation "},{"title":"Azure Cosmos DB (SQL API)​","type":1,"pageTitle":"Database","url":"express/Topics/Database#azure-cosmos-db-sql-api","content":"@azure/cosmos - Azure Cosmos DB "},{"title":"HTTP Security","type":0,"sectionRef":"#","url":"express/Topics/HTTP-Security","content":"HTTP Security There are many ways to secure your HTTP Traffic. In this section we will explore some tools/middlewares we can use to secure the incoming traffic. info It's not a silver bullet, but these can help you secure your traffic a bit! This template implements these following security measures: Security Headers: helmet - Helmet is a tool to help you secure your Express app by applying various HTTP headers aimed at securing your API against known HTTP vulnerabilities.HTTP Parameter Pollution (HPP): hpp - HTTP Parameter Pollution, or HPP, is a vulnerability that occurs due to the passing of multiple parameters having the same name. There is no RFC standard on what should be done when it has passed multiple parameters. This vulnerability was first discovered in 2009. HPP could be used for cross channel pollution, bypassing CSRF protection and WAF input validation checks.HTTP Content Length validation: express-content-length-validator - To not have your application vulnerable to large payload attacks you can restrict the maximum allowed payload size for incoming requests.","keywords":""},{"title":"Express","type":0,"sectionRef":"#","url":"express/Topics/Express","content":"Express Express is a web framework for Node that allows us to create a HTTP server where we can create our API. Getting started with an Express app is fairly easy and can be done by a couple of lines of code. This is great if you want to test something, but larger enterprise applications require a more robust structural layered design. You can find the easy to follow getting started guide at Express' homepage: &quot;Express Hello World&quot;. In this template i'll follow a multi-layered API architecture which divides the API into 3 layers: Routes and Controllers - This is where you handle the incoming requestsServices - Where you execute business logic on the entity, e.g. fetching other entities, calculating stuff.Repositories and Providers - Where you execute requests and/or queries to the database. NOTE: In this architecture it is important that the communication between each layer always flows downwards or sideways, never upwards. Like a service calling an controller. Project folder structure This repository contains an example of an multilayered API design structure, as well as an example API for an User entity. 📂 src ┣ 📂 api ┃ ┗ 📂 user ┃ ┣ 📂 dto ┃ ┃ ┣ 📜 create-user.dto.ts // DTO for creating a User ┃ ┃ ┣ 📜 update-user.dto.ts // DTO for updating a User ┃ ┃ ┗ 📜 user.dto.ts // Main DTO for a User ┃ ┣ 📂 entities ┃ ┃ ┗ 📜 user.entity.ts // Database entity for a User ┃ ┣ 📜 user.router.ts // Express router for handling incoming requests ┃ ┣ 📜 user.controller.ts // Controller class for handling incoming request from the router, only simple data validation is done at this level ┃ ┗ 📜 user.service.ts // Service class for handling business logic, like database queries, external HTTP requests etc ┃ ┣ 📂 __tests__ // Folder for test environment ┃ ┣ 📂 config // Folder for various configuration files ┣ 📂 middlewares // This is where you store all the custom express middlewares ┣ 📂 interfaces // This is where you define all typescript interfaces, types, enums etc. ┣ 📂 static // Folder for hosting static files. ┗ 📂 utils // Other utility functions. info To get you started this template provides a User entity with CRUD routes, controller, service and a repository for this entity.","keywords":""},{"title":"Testing with Jest","type":0,"sectionRef":"#","url":"express/Topics/Jest","content":"Testing with Jest This template is configured and setup with Jest for easy test-functionality. info By running the test:cov script Jest will generate coverage result in an format that can be shown in GitLab and SonarQube This config uses multiple external packages to work, please add the following dependencies to your project: @types/jest - Typings for Jestjest - Main Jest dependencyts-jest - A Jest transformer with source map support that lets you use Jest to test projects written in TypeScript.jest-junit - A Jest reporter that creates compatible JUnit XML files.","keywords":""},{"title":"Swagger","type":0,"sectionRef":"#","url":"express/Topics/Swagger","content":"Swagger One of the most important parts of an application is its documentation. Swagger and the OpenAPI Specification has become the de facto standard for API documentation. At Intility there is a requirement for API documentation using Swagger. Writing an OpenAPI documentation is a tedious task, both to write the first time as well as maintaining and updating. So in this template we'll be using the express-jsdoc-swagger for autogenerating the OpenAPI documentation as well as hosting the Swagger page on our API. info There is currently an issue with the Authorization flow built into Swagger doc. There fore we will be hosting our own redirect logic. This can be seen in the src/static folder The Swagger doc will be hosted at &lt;BASE_URL&gt;/api-docs. This can be changed in the swaggerOptions object in config/swagger.ts. This might break the default redirection logic provided by us. For an example on how to provide documentation for endpoints and interfaces. Have a look at the example routes and interfaces provided at routes/ and interfaces/. For more information and configuration options, head over to the Official Documentation","keywords":""},{"title":"FastAPI Template","type":0,"sectionRef":"#","url":"fastapi/","content":"FastAPI Template This FastAPI template is intended for you to get a good baseline for creating your FastAPI applications, with Azure AD authentication and authorization. The template has a lot of batteries included, such as pre-commit, project structure, example tests and options to use SQLModel and mypy. Before attempting to use this template, it is highly recommended you do the official tutorial. If you've created APIs in other Python web frameworks, FastAPI differs from the others by: FastAPI is built on starlette, so it's async firstAPI first, MVC second OpenAPI (aka Swagger) documentation are automatically generated! Fully type annotated, and types are used on runtimeFastAPI has very few batteries included (compared to e.g. Django). That's where this template is handy.","keywords":""},{"title":"Logging","type":0,"sectionRef":"#","url":"express/Topics/Logging","content":"","keywords":""},{"title":"How to use​","type":1,"pageTitle":"Logging","url":"express/Topics/Logging#how-to-use","content":"Create a new instance of LoggerService anywhere in your code and give it an label. From that instance you have access to all the different logging levels. const logger = new LoggerService('Server'); logger.error('Unable to connect to external service'); logger.warn('Skipping external request'); logger.info('Configuring service!'); logger.verbose('Created new entity'); logger.debug('Working?'); logger.silly('Just a silly message');   NOTE: By default logging levels are set to silly in non production environments and warn in production "},{"title":"How to customize​","type":1,"pageTitle":"Logging","url":"express/Topics/Logging#how-to-customize","content":"Winston is a very dynamic framework and can be customized to emit logs to multiple transports. There are several core transports included in winston, which leverage the built-in networking and file I/O offered by Node.js core. In addition, there are additional transports written by members of the community. For other customizations head over to the Official Documentation at GitHub. "},{"title":"Included Dependencies","type":0,"sectionRef":"#","url":"fastapi/getting-started/included-dependencies","content":"","keywords":""},{"title":"fastapi-azure-auth​","type":1,"pageTitle":"Included Dependencies","url":"fastapi/getting-started/included-dependencies#fastapi-azure-auth","content":"Docs Included for projects that select FastAPI Azure Auth (default) as their authentication strategy. This project is written and maintained by Intility. "},{"title":"sentry-sdk​","type":1,"pageTitle":"Included Dependencies","url":"fastapi/getting-started/included-dependencies#sentry-sdk","content":"Docs Used for error reporting to Sentry. "},{"title":"pre-commit​","type":1,"pageTitle":"Included Dependencies","url":"fastapi/getting-started/included-dependencies#pre-commit","content":"Docs Pre-commit is a code-checker that will run every time you attempt to commit any changes. It runs different programs, such as .flake8, black etc., all of which can be configured individually through either config files or the pyproject.toml. pre-commit When pre-commit changes something, you must add the modified file before you rerun pre-commit. It only runs on files staged for commit. "},{"title":"flake8​","type":1,"pageTitle":"Included Dependencies","url":"fastapi/getting-started/included-dependencies#flake8","content":"Docs Static code analysis which looks for different things, such as leftover print statements etc. "},{"title":"black​","type":1,"pageTitle":"Included Dependencies","url":"fastapi/getting-started/included-dependencies#black","content":"Docs Formats your code. Configuration is limited but can be found in pyproject.toml. "},{"title":"Installation","type":0,"sectionRef":"#","url":"fastapi/getting-started/installation","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Installation","url":"fastapi/getting-started/installation#prerequisites","content":"Some experience with PythonPython3.9+PoetryDocker and Docker ComposeGit "},{"title":"Intility Cruft​","type":1,"pageTitle":"Installation","url":"fastapi/getting-started/installation#intility-cruft","content":"You'll need cruft in order to use this template. python -m pip install cruft  cruft cruft is a templating language built on top of cookiecutter, which uses jinja templates. "},{"title":"Create FastAPI App​","type":1,"pageTitle":"Installation","url":"fastapi/getting-started/installation#create-fastapi-app","content":"Use cruft to start your project. python -m cruft create https://github.com/Intility/templates.git --directory=&quot;fastapi&quot;  A few questions you will be prompted, here with a more verbose description: full_name: Required for pyproject.tomlemail: Required for pyproject.tomlproject_name: The folder name, and required for pyproject.toml. Keep it short and simple!description: Required for pyproject.tomlsqlmodel: An ORM for FastAPI. Select 1 if you need a database. This will create example APIs, install alembic for migrations, add postgres to the docker-compose file etc.authentication_strategy: Every API must have authentication through Azure AD, but which strategy you chose is up to you. The options are: FastAPI Azure Auth (default): Authentication and authorization is done in your FastAPI application. This is the default and recommended authentication strategy, which this tutorial will assume you chose. Under the hood, this uses fastapi-azure-auth, a library written and maintained by Intility.Kong (no auth included): The alternative is to use Kong. This removes all authentication from your FastAPI application, and assume you host your application behind Kong. This tutorial will not cover this strategy. include_example_apis: Chose whether you want example APIs and tests to be included in the generated files. If this is the first time using this template, select true. You'll now have a new folder, matching your project_name. "},{"title":"Project Overview","type":0,"sectionRef":"#","url":"fastapi/getting-started/project-overview","content":"Project Overview Depending on which template you chose, your project structure will look more or less like the following: # __init__.py files has been removed from this visualization ├── alembic.ini # Configuration for your migrations ├── app # This is where your FastAPI lives │ ├── api # APIs and routes │ │ ├── api_v1 │ │ │ ├── api.py # API routes │ │ │ ├── endpoints # Yor API endpoints │ │ │ └── └── views.py # Example APIs, based on your options │ │ ├── dependencies.py # Dependencies for your application │ │ └── security.py # Security for your application, such as Azure AD authentication │ ├── core │ │ ├── db.py # Database engine is configured here │ │ └── config.py # All your application settings, such as secrets and passwords │ ├── main.py # FastAPI is configured here. │ ├── models # Database models │ │ └── cars.py │ └── schemas # Pydantic models │ └── hello_world.py ├── ci │ └── docker │ └── fastapi │ ├── Dockerfile │ ├── entrypoint.sh # Called when your Docker container is started │ └── gunicorn.conf.py # Basic gunicorn config ├── docker-compose.yaml ├── migrations # Autogenerated migrations are put here │ ├── alembic_functions.py │ ├── env.py # Environment for migrations. You should import your models here. │ ├── README # Read it, seriously. :) │ ├── script.py.mako │ └── versions │ └── d8ba1a4996fa_init.py # This is the first migration file ├── mypy.ini # mypy configuration files ├── poetry.lock # Autogenerated file for poetry ├── pyproject.toml # Your poetry configuration, such as dependencies ├── pytest.ini # Settings for all your tests └── tests # All tests are put here ├── api │ ├── auth_utils.py │ ├── conftest.py │ ├── test_api_with_auth.py │ └── test_cars_api.py ├── conftest.py # Shared utilities for your tests ├── models │ └── test_cars.py └── README.md # Read this too :) ","keywords":""},{"title":"Azure configuration","type":0,"sectionRef":"#","url":"fastapi/setup/authorization","content":"","keywords":""},{"title":"Backend API​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#backend-api","content":""},{"title":"Step 1 - Create app registration​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-1---create-app-registration","content":"Head over toAzure -&gt; Azure Active Directory -&gt; App registrations, and create a new registration. Select a fitting name for your project; Azure will present the name to the user during consent. Supported account types: Single tenant - If you want to create a multi-tenant application, you should head over to the official documentationRedirect URI: Leave blank. Press Register  "},{"title":"Step 2 - Change token version to v2​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-2---change-token-version-to-v2","content":"First we'll change the token version to version 2. In the left menu bar, click Manifest and find the line that says accessTokenAcceptedVersion. Change its value from null to 2. Press Save (This change can take some time to happen, which is why we do this first.)  "},{"title":"Step 3 - Note down your application IDs​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-3---note-down-your-application-ids","content":"Go back to the Overview, found in the left menu. Copy the Application (Client) ID place it in your .env file: .env TENANT_ID=9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6 APP_CLIENT_ID= OPENAPI_CLIENT_ID=   "},{"title":"Step 4 - Add an application scope​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-4---add-an-application-scope","content":"Go to Expose an API in the left menu bar under your app registration.Press + Add a scopeYou'll be prompted to set an Application ID URI, leave the suggested one and press Save and continue  Add a scope named user_impersonation that can be consented by Admins and users. You can use the following descriptions: Access API as user Allows the app to access the API as the user. Access API as you Allows the app to access the API as you.   "},{"title":"OpenAPI Documentation​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#openapi-documentation","content":"Our OpenAPI documentation will use the Authorization Code Grant Flow, with Proof Key for Code Exchange flow. It's a flow that enables a user of a Single-Page Application to safely log in, consent to permissions and fetch an access_tokenin the JWT format. When the user clicks Try out on the APIs, the access_token is attached to the header as a Bearer token. This is the token the backend will validate. So, let's set it up! "},{"title":"Step 1 - Create app registration​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-1---create-app-registration-1","content":"Just like in the previous chapter, we have to create an application registration for our OpenAPI. Head over toAzure -&gt; Azure Active Directory -&gt; App registrations, and create a new registration. Use the same name, but with - OpenAPI appended to it. Supported account types: Single tenantRedirect URI: Choose Single-Page Application (SPA) and http://localhost:8000/oauth2-redirect as a value Press Register  "},{"title":"Step 2 - Change token version to v2​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-2---change-token-version-to-v2-1","content":"Like last time, we'll change the token version to version 2. In the left menu bar, click Manifest and find the line that says accessTokenAcceptedVersion. Change its value from null to 2. Press Save  "},{"title":"Step 3 - Note down your application IDs​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-3---note-down-your-application-ids-1","content":"Go back to the Overview, found in the left menu. Copy the Application (Client) ID and save it as your OPENAPI_CLIENT_ID: .env TENANT_ID=9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6 APP_CLIENT_ID= OPENAPI_CLIENT_ID=   "},{"title":"Step 4 - Allow OpenAPI to talk to the backend​","type":1,"pageTitle":"Azure configuration","url":"fastapi/setup/authorization#step-4---allow-openapi-to-talk-to-the-backend","content":"To allow OpenAPI to talk to the backend API, you must add API permissions to the OpenAPI app registration. In the left menu, go to API Permissions and Add a permission.  Select the user_impersonation scope, and press Add a permission. Your view should now look something like this:  That's it! With these environment variables, FastAPI has been correctly configured from the template. "},{"title":"Docker-compose","type":0,"sectionRef":"#","url":"fastapi/setup/docker-compose","content":"","keywords":""},{"title":"Docker-compose​","type":1,"pageTitle":"Docker-compose","url":"fastapi/setup/docker-compose#docker-compose","content":"info Only projects that are generated with SQLModel will will have a docker-compose.yaml file. docker-compose is configured to run docker images such as redis for your development server, and for tests. Before we can run these containers, we need to configure our environment variables. Head over to your .env file and fill in the remaining values (highlighted). These are only for development use, but it is still recommended to generate a secure password. danger Your password should not contain slashes (/), since these passwords are used in connection strings. You can generate passwords by using this command: openssl rand -hex 32. .env # Basics ENVIRONMENT=dev # Sentry SENTRY_DSN= # Authentication TENANT_ID=9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6 APP_CLIENT_ID= OPENAPI_CLIENT_ID= # Only needed if you selected SQLModel POSTGRES_PASSWORD=  Then, run docker-compose: docker-compose up  "},{"title":"Inspecting our OpenAPI docs","type":0,"sectionRef":"#","url":"fastapi/setup/inspecting","content":"Inspecting our OpenAPI docs Now that we have a running application, we can head over and check out our OpenAPI documentation, and confirm that Azure AD Authentication works. Run the application, either throughpoetry or throughPyCharm. Head over to http://localhost:8000/docs. Here you'll see your API documentation, generated automatically frompydantic models. Try clicking on an API and try it out: As we can see, the response body is: { &quot;detail&quot;: &quot;Not authenticated&quot; } This is because this specific API view requires authentication. Click the Authorize button, leave the client_secret blank, and authorize. Log in through Azure AD, and try out the API again, and it should work. The next API, api/v1/hello-admin requires the user to have the AdminUser role. You can read more about roles in the official documentation, and how to lock down views on them in the FastAPI-Azure-Auth documentation.","keywords":""},{"title":"GitLab Repository","type":0,"sectionRef":"#","url":"fastapi/setup/gitlab","content":"","keywords":""},{"title":"Pushing your application​","type":1,"pageTitle":"GitLab Repository","url":"fastapi/setup/gitlab#pushing-your-application","content":"Creating a FastAPI project from a template does not automatically create a git repository for you, so we have to make one ourselves. pre-commit Remember, we use pre-commit, so we'll have to initiate that before we try to commit! Follow the &quot;Push an existing folder&quot; example, but mix in pre-commit. The commands should look like this: git init git remote add origin git@gitlab.intility.no:Group/intility-api.git git checkout -b main pre-commit install # &lt;--- This is an important step git add . git commit -m &quot;Initial commit&quot;  Sometimes checks will fail, or some files might be modified (by e.g. black). When this happens, add the modified files and try again: git add . git commit -m &quot;Initial commit&quot; git push origin main  Later, when you have made changes to your code, other checks may fail. These checks can be anything from missing docstrings, leftover prints etc. If you want to ignore checks, you can typically do so globally in the configuration files, or the line where the check fail. How to configure these checks will vary, so spend some time to get familiar with the tools. If you want to ignore pre-commit checks, you can use the --no-verify flag when committing: git add . git commit -m &quot;Initial commit&quot; --no-verify  "},{"title":"Without PyCharm (poetry)","type":0,"sectionRef":"#","url":"fastapi/setup/poetry","content":"","keywords":""},{"title":"Virtual environment​","type":1,"pageTitle":"Without PyCharm (poetry)","url":"fastapi/setup/poetry#virtual-environment","content":"There are two options for creating your virtual environment, either manually through poetry or PyCharm. info This section will cover how to use poetry. Head over to the previous page if you want to use PyCharm instead. "},{"title":"Creating a virtual environment​","type":1,"pageTitle":"Without PyCharm (poetry)","url":"fastapi/setup/poetry#creating-a-virtual-environment","content":"info Its easier to deal with poetry when the virtual environment is placed in your working directory. You can configure poetry to do this by issuing this command: poetry config virtualenvs.in-project true  First create a virtual environment and install dependencies: poetry update  The update command both updates the poetry.lock file and creates the virtual environment. Activate your new virtual environment: poetry shell  "},{"title":"Migrations​","type":1,"pageTitle":"Without PyCharm (poetry)","url":"fastapi/setup/poetry#migrations","content":"You can skip this section if you didn't select sqlmodel and have a database. In order to configure the database tables to reflect your models, you must run migrations. In this project, we use alembic to manage our migrations. Running migrations is done through the terminal: alembic upgrade head  "},{"title":"Running the server​","type":1,"pageTitle":"Without PyCharm (poetry)","url":"fastapi/setup/poetry#running-the-server","content":"With your virtual environment activated and docker-compose running, we can run our server. poetry run uvicorn app.main:app --host localhost --port 8000 --reload  "},{"title":"Sentry","type":0,"sectionRef":"#","url":"fastapi/setup/sentry","content":"","keywords":""},{"title":"Create Project​","type":1,"pageTitle":"Sentry","url":"fastapi/setup/sentry#create-project","content":"Head over to the Create a new Project page in Sentry. Under platform, select ASGI  For the project name, use the project slug from GitLab. Select a fitting team, or create a new one, and hit Create. You'll be taken to a Configure ASGI page. In the code examples, copy the DSN value passed to sentry_sdk.init, and add it to your .env file under SENTRY_DSN: .env # Basics ENVIRONMENT=dev # Sentry SENTRY_DSN= # Authentication TENANT_ID=9b5ff18e-53c0-45a2-8bc2-9c0c8f60b2c6 APP_CLIENT_ID= OPENAPI_CLIENT_ID= # Databases, cache etc. # Only needed if you selected SQLModel POSTGRES_PASSWORD=  "},{"title":"Using PyCharm","type":0,"sectionRef":"#","url":"fastapi/setup/pycharm","content":"","keywords":""},{"title":"Virtual environment​","type":1,"pageTitle":"Using PyCharm","url":"fastapi/setup/pycharm#virtual-environment","content":"There are two options for creating your virtual environment, either manually through poetry or PyCharm. info This section will cover how to use PyCharm. Head over to the next page if you want to use poetry instead. "},{"title":"Creating a virtual environment​","type":1,"pageTitle":"Using PyCharm","url":"fastapi/setup/pycharm#creating-a-virtual-environment","content":"Open PyCharm settings and search for interpreter  Create a new interpreter from your Python:  Open a new terminal in PyCharm (it will automatically be activated and install the dependencies: poetry update  The update command both updates the poetry.lock file and installs the dependencies. "},{"title":"Migrations​","type":1,"pageTitle":"Using PyCharm","url":"fastapi/setup/pycharm#migrations","content":"You can skip this section if you didn't select sqlmodel and have a database. In order to configure the database tables to reflect your models, you must run migrations. In this project, we use alembic to manage our migrations. Running migrations is done through the terminal: alembic upgrade head  "},{"title":"Configuring PyCharm to run the server​","type":1,"pageTitle":"Using PyCharm","url":"fastapi/setup/pycharm#configuring-pycharm-to-run-the-server","content":"Configure PyCharm to use uvicorn locally, pointing to your FastAPI application. It's important to run this on localhost:8000 and not 127.0.0.1:8000 for OpenAPI authentication to work properly. The --reload flag will ensure the server is restarted on every file change in the app folder. First, click the drop down in the top right corner.  Click the + sign and select Python.   Then configure PyCharm: Name: ServerModule name: uvicornParameters: app.main:app --host localhost --port 8000 --reloadPython interpreter: The one you created earlierWorking directory: Your current directory  Click OK to save and exit. Run the server. info Remember to run docker-compose before attempting to run your server. "},{"title":"SQLmodel and migrations","type":0,"sectionRef":"#","url":"fastapi/topics/migrations","content":"SQLmodel and migrations If you selected SQLModel you should do the official tutorial before making changes. When you create or change a SQLModel in Python, the database needs to be updated as well. We do this by generating (and potentially manually edit) migration files. This project has been shipped withalembic based on the testdriven.io tutorial.","keywords":""},{"title":"Testing","type":0,"sectionRef":"#","url":"fastapi/topics/testing","content":"Testing Testing is important. Your app is shipped with a tests folder, which contains a bunch of example tests. In order to understand them, you should read these resources: Official docs on testingOfficial docs on testing a databaseOfficial docs on async tests And lastly, learn about pytest, specifically how conftest.py and fixtures work. You can do so here: Official docs on pytest fixturesOfficial docs on pytest conftest.py When running tests, remember to first run your containers by running docker-compose up. You can now run pytest through poetry: poetry run pytest . --cov You can also run tests through PyCharm. Right click your tests folder and click Run 'pytest in tests'. If you're facing issues, click the dropdown in the top right corner and ensure the tests are run from your root folder.","keywords":""}]